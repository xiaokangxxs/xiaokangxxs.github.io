{"title":"RDD常用算子","slug":"RDD常用算子","date":"2021-04-24T08:36:47.813Z","updated":"2021-12-05T11:20:32.535Z","comments":true,"path":"api/articles/RDD常用算子.json","realPath":null,"excerpt":"RDD常用算子，赶紧get~","covers":["https://gitee.com/heibaiying/BigData-Notes/raw/master/pictures/spark-aggregateByKey.png"],"cover":"http://file.xiaokang.cool/colorful/colorful-4.jpg","content":"<h2 id=\"一、Transformation\"><a href=\"#一、Transformation\" class=\"headerlink\" title=\"一、Transformation\"></a>一、Transformation</h2><p>Spark常用的Transformation算子如下表：</p>\n<table>\n<thead>\n<tr>\n<th>Transformation 算子</th>\n<th>Meaning（含义）</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>map</strong>(<em>func</em>)</td>\n<td>对原RDD中每个元素运用<em>func</em>函数，并生成新的RDD</td>\n</tr>\n<tr>\n<td><strong>filter</strong>(<em>func</em>)</td>\n<td>对原RDD中每个元素使用<em>func</em>函数进行过滤，并生成新的RDD</td>\n</tr>\n<tr>\n<td><strong>flatMap</strong>(<em>func</em>)</td>\n<td>与map类似，但是每一个输入的item被映射成0个或多个输出的items（ <em>func</em>返回类型需要为Seq）。</td>\n</tr>\n<tr>\n<td><strong>mapPartitions</strong>(<em>func</em>)</td>\n<td>与map类似，但函数单独在RDD的每个分区上运行， <em>func</em>函数的类型为*Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt;*，其中T是RDD的类型，即RDD[T]</td>\n</tr>\n<tr>\n<td><strong>mapPartitionsWithIndex</strong>(<em>func</em>)</td>\n<td>与mapPartitions类似，但<em>func</em>类型为 (Int,Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt;，其中第一个参数为分区索引</td>\n</tr>\n<tr>\n<td><strong>sample</strong>(<em>withReplacement</em>, <em>fraction</em>, <em>seed</em>)</td>\n<td>数据采样，有三个可选参数：设置是否放回（withReplacement）、采样的百分比（<em>fraction</em>）、随机数生成器的种子（seed）。</td>\n</tr>\n<tr>\n<td><strong>union</strong>(<em>otherDataset</em>)</td>\n<td>合并两个RDD</td>\n</tr>\n<tr>\n<td><strong>intersection</strong>(<em>otherDataset</em>)</td>\n<td>求两个RDD的交集</td>\n</tr>\n<tr>\n<td><strong>distinct</strong>([<em>numTasks</em>]))</td>\n<td>去重</td>\n</tr>\n<tr>\n<td><strong>groupByKey</strong>([<em>numTasks</em>])</td>\n<td>按照key值进行分区，即在一个 (K, V) 对的dataset上调用时，返回一个 (K, Iterable&lt;V&gt;) <strong>Note:</strong> 如果分组是为了在每一个key上执行聚合操作（例如，sum 或 average)，此时使用 <code>reduceByKey</code> 或<code>aggregateByKey</code> 性能会更好<strong>Note:</strong> 默认情况下，并行度取决于父 RDD 的分区数。可以传入 <code>numTasks</code> 参数进行修改。</td>\n</tr>\n<tr>\n<td><strong>reduceByKey</strong>(<em>func</em>, [<em>numTasks</em>])</td>\n<td>按照key值进行分组，并对分组后的数据执行归约操作（于groupByKey相比，reduceByKey会有提前combine操作）。</td>\n</tr>\n<tr>\n<td><strong>aggregateByKey</strong>(<em>zeroValue</em>,<em>numPartitions</em>)(<em>seqOp</em>, <em>combOp</em>, [<em>numTasks</em>])</td>\n<td>当调用（K，V）对的数据集时，返回（K，U）对的数据集，其中使用给定的组合函数和zeroValue聚合每个键的值。与groupByKey类似，reduce任务的数量可通过第二个参数进行配置。</td>\n</tr>\n<tr>\n<td><strong>sortByKey</strong>([<em>ascending</em>], [<em>numTasks</em>])</td>\n<td>按照key进行排序，其中的key需要实现Ordered特质，即可比较</td>\n</tr>\n<tr>\n<td><strong>join</strong>(<em>otherDataset</em>, [<em>numTasks</em>])</td>\n<td>在一个 (K, V) 和 (K, W) 类型的dataset上调用时，返回一个 (K, (V, W)) pairs 的dataset，等价于内连接操作。如果想要执行外连接，可以使用 <code>leftOuterJoin</code>, <code>rightOuterJoin</code> 和 <code>fullOuterJoin</code> 等算子。</td>\n</tr>\n<tr>\n<td><strong>cogroup</strong>(<em>otherDataset</em>, [<em>numTasks</em>])</td>\n<td>在一个 (K, V) 对的dataset上调用时，返回一个 (K, (Iterable&lt;V&gt;, Iterable&lt;W&gt;)) tuples的dataset。</td>\n</tr>\n<tr>\n<td><strong>cartesian</strong>(<em>otherDataset</em>)</td>\n<td>在一个T和U类型的dataset上调用时，返回一个 (T, U) 类型的dataset（即笛卡尔积）。</td>\n</tr>\n<tr>\n<td><strong>coalesce</strong>(<em>numPartitions</em>)</td>\n<td>将RDD中的分区数减少为numPartitions。</td>\n</tr>\n<tr>\n<td><strong>repartition</strong>(<em>numPartitions</em>)</td>\n<td>随机重新调整RDD中的数据以创建更多或更少的分区，并在它们之间进行平衡。</td>\n</tr>\n<tr>\n<td><strong>repartitionAndSortWithinPartitions</strong>(<em>partitioner</em>)</td>\n<td>根据给定的partitioner（分区器）对RDD进行重新分区，并对分区中的数据按照key值进行排序。这比调用 <code>repartition</code> 然后再sorting（排序）效率更高，因为它可以将排序过程推送到shuffle操作所在的机器。</td>\n</tr>\n</tbody></table>\n<p>下面给出部分算子的基本使用示例：</p>\n<h3 id=\"1-1-map\"><a href=\"#1-1-map\" class=\"headerlink\" title=\"1.1 map\"></a>1.1 map</h3><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">4</span>)<br>scala&gt; sc.parallelize(list).map(_ * <span class=\"hljs-number\">2</span>).foreach(println)<br><span class=\"hljs-number\">2</span><br><span class=\"hljs-number\">2</span><br><span class=\"hljs-number\">4</span><br><span class=\"hljs-number\">8</span><br></code></pre></td></tr></table></figure>\n\n<h3 id=\"1-2-filter\"><a href=\"#1-2-filter\" class=\"headerlink\" title=\"1.2 filter\"></a>1.2 filter</h3><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">4</span>)<br>scala&gt; sc.parallelize(list).filter(_ &gt;= <span class=\"hljs-number\">2</span>).foreach(println)<br><span class=\"hljs-number\">2</span><br><span class=\"hljs-number\">4</span><br></code></pre></td></tr></table></figure>\n\n<h3 id=\"1-3-flatMap\"><a href=\"#1-3-flatMap\" class=\"headerlink\" title=\"1.3 flatMap\"></a>1.3 flatMap</h3><p><code>flatMap(func)</code> 与 <code>map</code> 类似，但每一个输入的item会被映射成0个或多个输出的items（<em>func</em>返回类型需要为<code>Seq</code>）。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>(<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>), <span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">4</span>), <span class=\"hljs-type\">List</span>(), <span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">4</span>,<span class=\"hljs-number\">6</span>,<span class=\"hljs-number\">8</span>))<br>scala&gt; sc.parallelize(list).flatMap(_.toList).foreach(println)<br><span class=\"hljs-number\">1</span><br><span class=\"hljs-number\">2</span><br><span class=\"hljs-number\">4</span><br><span class=\"hljs-number\">4</span><br><span class=\"hljs-number\">6</span><br><span class=\"hljs-number\">8</span><br></code></pre></td></tr></table></figure>\n\n<p>flatMap在日志分析中使用概率非常高，这里进行一下演示：拆分输入的每行数据为单个单词，并赋值为1，代表出现一次，之后按照单词分组并统计其出现总次数，代码如下：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> lines=<span class=\"hljs-type\">List</span>(<span class=\"hljs-string\">&quot;spark flume xiaokang spark scala&quot;</span>,<span class=\"hljs-string\">&quot;hadoop scala xiaokangxxs xiaokang flume hive&quot;</span>)<br>scala&gt; sc.parallelize(lines).flatMap(_.split(<span class=\"hljs-string\">&quot; &quot;</span>)).map((_,<span class=\"hljs-number\">1</span>)).reduceByKey(_+_).foreach(println)<br>(scala,<span class=\"hljs-number\">2</span>)<br>(xiaokangxxs,<span class=\"hljs-number\">1</span>)<br>(spark,<span class=\"hljs-number\">2</span>)<br>(hive,<span class=\"hljs-number\">1</span>)<br>(hadoop,<span class=\"hljs-number\">1</span>)<br>(flume,<span class=\"hljs-number\">2</span>)<br>(xiaokang,<span class=\"hljs-number\">2</span>)<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"1-4-mapPartitions\"><a href=\"#1-4-mapPartitions\" class=\"headerlink\" title=\"1.4 mapPartitions\"></a>1.4 mapPartitions</h3><p>与map类似，但函数单独在RDD的每个分区上运行， <em>func</em>函数的类型为<code>Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt;</code> (其中T是RDD的类型)，即输入和输出都必须是可迭代类型。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">import</span> scala.collection.mutable.<span class=\"hljs-type\">ListBuffer</span><br><span class=\"hljs-keyword\">import</span> scala.collection.mutable.<span class=\"hljs-type\">ListBuffer</span><br>scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">4</span>,<span class=\"hljs-number\">6</span>,<span class=\"hljs-number\">8</span>)<br>scala&gt; sc.parallelize(list, <span class=\"hljs-number\">3</span>).mapPartitions(iterator =&gt; &#123;<br>     |   <span class=\"hljs-keyword\">val</span> buffer = <span class=\"hljs-keyword\">new</span> <span class=\"hljs-type\">ListBuffer</span>[<span class=\"hljs-type\">Int</span>]<br>     |   <span class=\"hljs-keyword\">while</span> (iterator.hasNext) &#123;<br>     |      buffer.append(iterator.next()*<span class=\"hljs-number\">3</span>)<br>     |   &#125;<br>     |   buffer.toIterator&#125;).foreach(println)<br><span class=\"hljs-number\">3</span><br><span class=\"hljs-number\">3</span><br><span class=\"hljs-number\">6</span><br><span class=\"hljs-number\">12</span><br><span class=\"hljs-number\">18</span><br><span class=\"hljs-number\">24</span><br></code></pre></td></tr></table></figure>\n\n<h3 id=\"1-5-mapPartitionsWithIndex\"><a href=\"#1-5-mapPartitionsWithIndex\" class=\"headerlink\" title=\"1.5 mapPartitionsWithIndex\"></a>1.5 mapPartitionsWithIndex</h3><p>  与mapPartitions类似，但<em>func</em>类型为 <code>(Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt;</code> ，其中第一个参数为分区索引。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">import</span> scala.collection.mutable.<span class=\"hljs-type\">ListBuffer</span><br><span class=\"hljs-keyword\">import</span> scala.collection.mutable.<span class=\"hljs-type\">ListBuffer</span><br>scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">4</span>,<span class=\"hljs-number\">6</span>,<span class=\"hljs-number\">8</span>)<br>scala&gt; sc.parallelize(list, <span class=\"hljs-number\">3</span>).mapPartitionsWithIndex((index,iterator) =&gt; &#123;<br>     |   <span class=\"hljs-keyword\">val</span> buffer = <span class=\"hljs-keyword\">new</span> <span class=\"hljs-type\">ListBuffer</span>[<span class=\"hljs-type\">String</span>]<br>     |   <span class=\"hljs-keyword\">while</span> (iterator.hasNext) &#123;<br>     |      buffer.append(index+<span class=\"hljs-string\">&quot;---&quot;</span>+iterator.next()*<span class=\"hljs-number\">3</span>)<br>     |   &#125;<br>     |   buffer.toIterator&#125;).foreach(println)<br><span class=\"hljs-number\">0</span>--<span class=\"hljs-number\">-3</span><br><span class=\"hljs-number\">0</span>--<span class=\"hljs-number\">-3</span><br><span class=\"hljs-number\">1</span>--<span class=\"hljs-number\">-6</span><br><span class=\"hljs-number\">1</span>--<span class=\"hljs-number\">-12</span><br><span class=\"hljs-number\">2</span>--<span class=\"hljs-number\">-18</span><br><span class=\"hljs-number\">2</span>--<span class=\"hljs-number\">-24</span><br></code></pre></td></tr></table></figure>\n\n<h3 id=\"1-6-sample\"><a href=\"#1-6-sample\" class=\"headerlink\" title=\"1.6 sample\"></a>1.6 sample</h3><p>数据采样。有三个可选参数：设置是否放回 (withReplacement)、采样的百分比 (fraction)、随机数生成器的种子 (seed) ：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">4</span>,<span class=\"hljs-number\">6</span>,<span class=\"hljs-number\">8</span>)<br>scala&gt; sc.parallelize(list).sample(withReplacement=<span class=\"hljs-literal\">false</span>,fraction=<span class=\"hljs-number\">0.5</span>).foreach(println)<br><span class=\"hljs-number\">1</span><br><span class=\"hljs-number\">1</span><br><span class=\"hljs-number\">4</span><br><span class=\"hljs-number\">6</span><br>scala&gt; sc.parallelize(list).sample(withReplacement=<span class=\"hljs-literal\">false</span>,fraction=<span class=\"hljs-number\">0.5</span>).foreach(println)<br><span class=\"hljs-number\">1</span><br><span class=\"hljs-number\">8</span><br>scala&gt; sc.parallelize(list).sample(withReplacement=<span class=\"hljs-literal\">false</span>,fraction=<span class=\"hljs-number\">0.5</span>).foreach(println)<br><span class=\"hljs-number\">1</span><br>scala&gt; sc.parallelize(list).sample(withReplacement=<span class=\"hljs-literal\">false</span>,fraction=<span class=\"hljs-number\">0.5</span>).foreach(println)<br><span class=\"hljs-number\">1</span><br><span class=\"hljs-number\">4</span><br><span class=\"hljs-number\">6</span><br></code></pre></td></tr></table></figure>\n\n<h3 id=\"1-7-union\"><a href=\"#1-7-union\" class=\"headerlink\" title=\"1.7 union\"></a>1.7 union</h3><p>合并两个RDD：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list1=<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">3</span>)<br>scala&gt; <span class=\"hljs-keyword\">val</span> list2=<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">6</span>,<span class=\"hljs-number\">8</span>,<span class=\"hljs-number\">10</span>)<br>scala&gt; sc.parallelize(list1).union(sc.parallelize(list2)).foreach(println)<br><span class=\"hljs-number\">1</span><br><span class=\"hljs-number\">2</span><br><span class=\"hljs-number\">3</span><br><span class=\"hljs-number\">6</span><br><span class=\"hljs-number\">8</span><br><span class=\"hljs-number\">10</span><br></code></pre></td></tr></table></figure>\n\n<h3 id=\"1-8-intersection\"><a href=\"#1-8-intersection\" class=\"headerlink\" title=\"1.8 intersection\"></a>1.8 intersection</h3><p>求两个 RDD 的交集：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list1=<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">3</span>,<span class=\"hljs-number\">6</span>)<br>scala&gt; <span class=\"hljs-keyword\">val</span> list2=<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">5</span>,<span class=\"hljs-number\">6</span>,<span class=\"hljs-number\">9</span>,<span class=\"hljs-number\">11</span>)<br>scala&gt; sc.parallelize(list1).intersection(sc.parallelize(list2)).foreach(println)<br><span class=\"hljs-number\">6</span><br></code></pre></td></tr></table></figure>\n\n<h3 id=\"1-9-distinct\"><a href=\"#1-9-distinct\" class=\"headerlink\" title=\"1.9 distinct\"></a>1.9 distinct</h3><p>去重：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">4</span>,<span class=\"hljs-number\">6</span>,<span class=\"hljs-number\">6</span>,<span class=\"hljs-number\">8</span>)<br>scala&gt; sc.parallelize(list).distinct().foreach(println)<br><span class=\"hljs-number\">4</span><br><span class=\"hljs-number\">1</span><br><span class=\"hljs-number\">6</span><br><span class=\"hljs-number\">8</span><br><span class=\"hljs-number\">2</span><br></code></pre></td></tr></table></figure>\n\n<h3 id=\"1-10-groupByKey\"><a href=\"#1-10-groupByKey\" class=\"headerlink\" title=\"1.10 groupByKey\"></a>1.10 groupByKey</h3><p>按照键进行分组：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>((<span class=\"hljs-string\">&quot;hadoop&quot;</span>, <span class=\"hljs-number\">2</span>), (<span class=\"hljs-string\">&quot;spark&quot;</span>, <span class=\"hljs-number\">3</span>), (<span class=\"hljs-string\">&quot;xiaokang&quot;</span>, <span class=\"hljs-number\">5</span>), (<span class=\"hljs-string\">&quot;spark&quot;</span>, <span class=\"hljs-number\">6</span>), (<span class=\"hljs-string\">&quot;hadoop&quot;</span>, <span class=\"hljs-number\">2</span>))<br>scala&gt; sc.parallelize(list).groupByKey().map(x =&gt; (x._1, x._2.toList)).foreach(println)<br>(spark,<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">3</span>, <span class=\"hljs-number\">6</span>))<br>(hadoop,<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">2</span>))<br>(xiaokang,<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">5</span>))<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"1-11-reduceByKey\"><a href=\"#1-11-reduceByKey\" class=\"headerlink\" title=\"1.11 reduceByKey\"></a>1.11 reduceByKey</h3><p>按照键进行归约操作：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>((<span class=\"hljs-string\">&quot;hadoop&quot;</span>, <span class=\"hljs-number\">2</span>), (<span class=\"hljs-string\">&quot;spark&quot;</span>, <span class=\"hljs-number\">3</span>), (<span class=\"hljs-string\">&quot;xiaokang&quot;</span>, <span class=\"hljs-number\">5</span>), (<span class=\"hljs-string\">&quot;spark&quot;</span>, <span class=\"hljs-number\">6</span>), (<span class=\"hljs-string\">&quot;hadoop&quot;</span>, <span class=\"hljs-number\">2</span>))<br>scala&gt; sc.parallelize(list).reduceByKey(_+_).foreach(println)<br>(spark,<span class=\"hljs-number\">9</span>)<br>(hadoop,<span class=\"hljs-number\">4</span>)<br>(xiaokang,<span class=\"hljs-number\">5</span>)<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"1-12-sortBy-amp-sortByKey\"><a href=\"#1-12-sortBy-amp-sortByKey\" class=\"headerlink\" title=\"1.12 sortBy &amp; sortByKey\"></a>1.12 sortBy &amp; sortByKey</h3><p>按照键进行排序：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>((<span class=\"hljs-number\">100</span>, <span class=\"hljs-string\">&quot;hadoop&quot;</span>), (<span class=\"hljs-number\">90</span>, <span class=\"hljs-string\">&quot;spark&quot;</span>), (<span class=\"hljs-number\">120</span>, <span class=\"hljs-string\">&quot;xiaokangxxs&quot;</span>))<br>scala&gt; sc.parallelize(list).sortByKey(ascending=<span class=\"hljs-literal\">false</span>).foreach(println)<br>(<span class=\"hljs-number\">120</span>,xiaokangxxs)<br>(<span class=\"hljs-number\">100</span>,hadoop)<br>(<span class=\"hljs-number\">90</span>,spark)<br></code></pre></td></tr></table></figure>\n\n<p>按照指定元素进行排序：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>((<span class=\"hljs-string\">&quot;hadoop&quot;</span>,<span class=\"hljs-number\">100</span>), (<span class=\"hljs-string\">&quot;spark&quot;</span>,<span class=\"hljs-number\">90</span>), (<span class=\"hljs-string\">&quot;xiaokangxxs&quot;</span>,<span class=\"hljs-number\">120</span>))<br>scala&gt; sc.parallelize(list).sortBy(x=&gt;x._2,ascending=<span class=\"hljs-literal\">false</span>).foreach(println)<br>(xiaokangxxs,<span class=\"hljs-number\">120</span>)<br>(hadoop,<span class=\"hljs-number\">100</span>)<br>(spark,<span class=\"hljs-number\">90</span>)<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"1-13-join\"><a href=\"#1-13-join\" class=\"headerlink\" title=\"1.13 join\"></a>1.13 join</h3><p>在一个 (K, V) 和 (K, W) 类型的Dataset上调用时，返回一个 (K, (V, W)) 的Dataset，等价于内连接操作。如果想要执行外连接，可以使用 <code>leftOuterJoin</code>, <code>rightOuterJoin</code> 和 <code>fullOuterJoin</code> 等算子。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list1=<span class=\"hljs-type\">List</span>((<span class=\"hljs-number\">1</span>,<span class=\"hljs-string\">&quot;student01&quot;</span>),(<span class=\"hljs-number\">2</span>,<span class=\"hljs-string\">&quot;student02&quot;</span>),(<span class=\"hljs-number\">3</span>,<span class=\"hljs-string\">&quot;student03&quot;</span>))<br>scala&gt; <span class=\"hljs-keyword\">val</span> list2=<span class=\"hljs-type\">List</span>((<span class=\"hljs-number\">1</span>,<span class=\"hljs-string\">&quot;teacher01&quot;</span>),(<span class=\"hljs-number\">2</span>,<span class=\"hljs-string\">&quot;teacher02&quot;</span>),(<span class=\"hljs-number\">3</span>,<span class=\"hljs-string\">&quot;teacher03&quot;</span>))<br>scala&gt; sc.parallelize(list1).join(sc.parallelize(list2)).foreach(println)<br>(<span class=\"hljs-number\">1</span>,(student01,teacher01))<br>(<span class=\"hljs-number\">3</span>,(student03,teacher03))<br>(<span class=\"hljs-number\">2</span>,(student02,teacher02))<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"1-14-cartesian\"><a href=\"#1-14-cartesian\" class=\"headerlink\" title=\"1.14 cartesian\"></a>1.14 cartesian</h3><p>计算笛卡尔积：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list1=<span class=\"hljs-type\">List</span>(<span class=\"hljs-string\">&quot;xiaokang&quot;</span>, <span class=\"hljs-string\">&quot;spark&quot;</span>, <span class=\"hljs-string\">&quot;flink&quot;</span>)<br>scala&gt; <span class=\"hljs-keyword\">val</span> list2=<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">3</span>)<br>scala&gt; sc.parallelize(list1).cartesian(sc.parallelize(list2)).foreach(println)<br>(xiaokang,<span class=\"hljs-number\">1</span>)<br>(xiaokang,<span class=\"hljs-number\">2</span>)<br>(xiaokang,<span class=\"hljs-number\">3</span>)<br>(spark,<span class=\"hljs-number\">1</span>)<br>(spark,<span class=\"hljs-number\">2</span>)<br>(spark,<span class=\"hljs-number\">3</span>)<br>(flink,<span class=\"hljs-number\">1</span>)<br>(flink,<span class=\"hljs-number\">2</span>)<br>(flink,<span class=\"hljs-number\">3</span>)<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"1-15-aggregateByKey\"><a href=\"#1-15-aggregateByKey\" class=\"headerlink\" title=\"1.15 aggregateByKey\"></a>1.15 aggregateByKey</h3><p>当调用（K，V）对的数据集时，返回（K，U）对的数据集，其中使用给定的组合函数和zeroValue聚合每个键的值。与<code>groupByKey</code>类似，reduce任务的数量可通过第二个参数<code>numPartitions</code>进行配置。示例如下：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>((<span class=\"hljs-string\">&quot;hadoop&quot;</span>,<span class=\"hljs-number\">3</span>), (<span class=\"hljs-string\">&quot;hadoop&quot;</span>,<span class=\"hljs-number\">2</span>), (<span class=\"hljs-string\">&quot;spark&quot;</span>,<span class=\"hljs-number\">4</span>), (<span class=\"hljs-string\">&quot;spark&quot;</span>,<span class=\"hljs-number\">3</span>), (<span class=\"hljs-string\">&quot;storm&quot;</span>,<span class=\"hljs-number\">6</span>), (<span class=\"hljs-string\">&quot;storm&quot;</span>,<span class=\"hljs-number\">8</span>))<br>scala&gt; sc.parallelize(list,numSlices = <span class=\"hljs-number\">2</span>).aggregateByKey(zeroValue = <span class=\"hljs-number\">0</span>,numPartitions = <span class=\"hljs-number\">3</span>)(<br>     |  seqOp=math.max(_, _),<br>     |  combOp=_ + _<br>     | ).collect.foreach(println)<br>(hadoop,<span class=\"hljs-number\">3</span>)<br>(storm,<span class=\"hljs-number\">8</span>)<br>(spark,<span class=\"hljs-number\">7</span>)<br></code></pre></td></tr></table></figure>\n\n<p>这里使用了<code>numSlices = 2</code>指定aggregateByKey父操作parallelize的分区数量为2，其执行流程如下：</p>\n<div align=\"center\"> <img src=\"https://gitee.com/heibaiying/BigData-Notes/raw/master/pictures/spark-aggregateByKey.png\"/> </div>\n基于同样的执行流程，如果`numSlices=1`，则意味着只有输入一个分区，则其最后一步combOp相当于是无效的，执行结果为：\n\n<figure class=\"highlight properties\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs properties\"><span class=\"hljs-attr\">(hadoop,3)</span><br><span class=\"hljs-attr\">(storm,8)</span><br><span class=\"hljs-attr\">(spark,4)</span><br></code></pre></td></tr></table></figure>\n\n<p>同样的，如果每个单词对一个分区，即<code>numSlices=6</code>，此时相当于求和操作，执行结果为：</p>\n<figure class=\"highlight properties\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs properties\"><span class=\"hljs-attr\">(hadoop,5)</span><br><span class=\"hljs-attr\">(storm,14)</span><br><span class=\"hljs-attr\">(spark,7)</span><br></code></pre></td></tr></table></figure>\n\n<p><code>aggregateByKey(zeroValue=0,numPartitions=3)</code>的第二个参数<code>numPartitions</code> 决定的是输出 RDD 的分区数量，想要验证这个问题，可以对上面代码进行改写，使用<code>getNumPartitions</code>方法获取分区数量：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; sc.parallelize(list,numSlices=<span class=\"hljs-number\">6</span>).aggregateByKey(zeroValue=<span class=\"hljs-number\">0</span>,numPartitions=<span class=\"hljs-number\">3</span>)(<br>     |   seqOp = math.max(_, _),<br>     |   combOp = _ + _<br>     | ).getNumPartitions<br>res16: <span class=\"hljs-type\">Int</span> = <span class=\"hljs-number\">3</span><br></code></pre></td></tr></table></figure>\n\n<h2 id=\"二、Action\"><a href=\"#二、Action\" class=\"headerlink\" title=\"二、Action\"></a>二、Action</h2><p>Spark常用的Action算子如下：</p>\n<table>\n<thead>\n<tr>\n<th>Action（动作）</th>\n<th>Meaning（含义）</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>reduce</strong>(<em>func</em>)</td>\n<td>使用函数<em>func</em>执行归约操作</td>\n</tr>\n<tr>\n<td><strong>collect</strong>()</td>\n<td>以一个array数组的形式返回dataset的所有元素，适用于小结果集。</td>\n</tr>\n<tr>\n<td><strong>count</strong>()</td>\n<td>返回RDD中元素的个数。</td>\n</tr>\n<tr>\n<td><strong>first</strong>()</td>\n<td>返回RDD中的第一个元素，不排序。</td>\n</tr>\n<tr>\n<td><strong>take</strong>(<em>n</em>)</td>\n<td>将RDD中的前<em>n</em>个元素作为一个array数组返回。</td>\n</tr>\n<tr>\n<td><strong>takeSample</strong>(<em>withReplacement</em>, <em>num</em>, [<em>seed</em>])</td>\n<td>对一个RDD进行随机抽样</td>\n</tr>\n<tr>\n<td><strong>takeOrdered</strong>(<em>n</em>, <em>[ordering]</em>)</td>\n<td>按自然顺序（natural order）或自定义比较器（custom comparator）排序后返回前<em>n</em>个元素。只适用于小结果集，因为所有数据都会被加载到驱动程序的内存中进行排序。</td>\n</tr>\n<tr>\n<td><strong>saveAsTextFile</strong>(<em>path</em>)</td>\n<td>将RDD中的元素以文本文件的形式写入本地文件系统、HDFS或其它 Hadoop支持的文件系统中。Spark将对每个元素调用toString方法，将元素转换为文本文件中的一行记录。</td>\n</tr>\n<tr>\n<td><strong>saveAsSequenceFile</strong>(<em>path</em>)</td>\n<td>将RDD中的元素以Hadoop SequenceFile的形式写入到本地文件系统、HDFS或其它Hadoop支持的文件系统中。该操作要求RDD中的元素需要实现Hadoop的Writable接口。对于Scala语言而言，它可以将Spark中的基本数据类型自动隐式转换为对应Writable类型。(目前仅支持Java and Scala)</td>\n</tr>\n<tr>\n<td><strong>saveAsObjectFile</strong>(<em>path</em>)</td>\n<td>使用Java序列化后存储，可以使用<code>SparkContext.objectFile()</code>进行加载。(目前仅支持Java and Scala)</td>\n</tr>\n<tr>\n<td><strong>countByKey</strong>()</td>\n<td>计算每个键出现的次数。</td>\n</tr>\n<tr>\n<td><strong>foreach</strong>(<em>func</em>)</td>\n<td>遍历RDD中每个元素，并对其执行<em>fun</em>函数</td>\n</tr>\n<tr>\n<td><strong>foreachPartition</strong>(<em>func</em>)</td>\n<td>与foreach类似，但函数单独在RDD的每个分区上运行，运用<em>func</em>函数，并生成新的RDD。</td>\n</tr>\n</tbody></table>\n<p>下面给出部分算子的基本使用示例：</p>\n<h3 id=\"2-1-take\"><a href=\"#2-1-take\" class=\"headerlink\" title=\"2.1 take\"></a>2.1 take</h3><p>将RDD中的前<em>n</em>个元素作为一个array数组返回：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">4</span>,<span class=\"hljs-number\">6</span>,<span class=\"hljs-number\">8</span>,<span class=\"hljs-number\">11</span>)<br>scala&gt; sc.parallelize(list).take(<span class=\"hljs-number\">3</span>)<br>res21: <span class=\"hljs-type\">Array</span>[<span class=\"hljs-type\">Int</span>] = <span class=\"hljs-type\">Array</span>(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>)<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"2-3-first\"><a href=\"#2-3-first\" class=\"headerlink\" title=\"2.3 first\"></a>2.3 first</h3><p>返回RDD中的第一个元素，不排序：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">4</span>,<span class=\"hljs-number\">6</span>,<span class=\"hljs-number\">8</span>,<span class=\"hljs-number\">11</span>)<br>scala&gt; sc.parallelize(list).first<br>res22: <span class=\"hljs-type\">Int</span> = <span class=\"hljs-number\">1</span><br></code></pre></td></tr></table></figure>\n\n<h3 id=\"2-4-reduce\"><a href=\"#2-4-reduce\" class=\"headerlink\" title=\"2.4 reduce\"></a>2.4 reduce</h3><p>使用函数<em>func</em>执行归约操作：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">3</span>,<span class=\"hljs-number\">4</span>,<span class=\"hljs-number\">5</span>,<span class=\"hljs-number\">6</span>,<span class=\"hljs-number\">7</span>,<span class=\"hljs-number\">8</span>,<span class=\"hljs-number\">9</span>,<span class=\"hljs-number\">10</span>)<br>scala&gt; sc.parallelize(list).reduce(_+_)<br>res18: <span class=\"hljs-type\">Int</span> = <span class=\"hljs-number\">55</span><br></code></pre></td></tr></table></figure>\n\n<h3 id=\"2-5-takeOrdered\"><a href=\"#2-5-takeOrdered\" class=\"headerlink\" title=\"2.5 takeOrdered\"></a>2.5 takeOrdered</h3><p>按自然顺序（natural order）或自定义比较器（custom comparator）排序后返回前 <em>n</em> 个元素。需要注意的是 <code>takeOrdered</code> 使用隐式参数进行隐式转换，以下为其源码。所以在使用自定义排序时，需要继承 <code>Ordering[T]</code> 实现自定义比较器，然后将其作为隐式参数引入。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\"><span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">takeOrdered</span></span>(num: <span class=\"hljs-type\">Int</span>)(<span class=\"hljs-keyword\">implicit</span> ord: <span class=\"hljs-type\">Ordering</span>[<span class=\"hljs-type\">T</span>]): <span class=\"hljs-type\">Array</span>[<span class=\"hljs-type\">T</span>] = withScope &#123;<br>  .........<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<p>自定义规则排序：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\"><span class=\"hljs-comment\">// 继承 Ordering[T],实现自定义比较器，按照 value 值的长度进行排序</span><br><span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">CustomOrdering</span> <span class=\"hljs-keyword\">extends</span> <span class=\"hljs-title\">Ordering</span>[(<span class=\"hljs-type\">Int</span>, <span class=\"hljs-type\">String</span>)] </span>&#123;<br>    <span class=\"hljs-keyword\">override</span> <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">compare</span></span>(x: (<span class=\"hljs-type\">Int</span>, <span class=\"hljs-type\">String</span>), y: (<span class=\"hljs-type\">Int</span>, <span class=\"hljs-type\">String</span>)): <span class=\"hljs-type\">Int</span><br>    = <span class=\"hljs-keyword\">if</span> (x._2.length &gt; y._2.length) <span class=\"hljs-number\">1</span> <span class=\"hljs-keyword\">else</span> <span class=\"hljs-number\">-1</span><br>&#125;<br></code></pre></td></tr></table></figure>\n\n<p>具体使用：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">CustomOrdering</span> <span class=\"hljs-keyword\">extends</span> <span class=\"hljs-title\">Ordering</span>[(<span class=\"hljs-type\">Int</span>, <span class=\"hljs-type\">String</span>)] </span>&#123;<br>     |     <span class=\"hljs-keyword\">override</span> <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">compare</span></span>(x: (<span class=\"hljs-type\">Int</span>, <span class=\"hljs-type\">String</span>), y: (<span class=\"hljs-type\">Int</span>, <span class=\"hljs-type\">String</span>)): <span class=\"hljs-type\">Int</span><br>     |     = <span class=\"hljs-keyword\">if</span> (x._2.length &gt; y._2.length) <span class=\"hljs-number\">1</span> <span class=\"hljs-keyword\">else</span> <span class=\"hljs-number\">-1</span><br>     | &#125;<br>defined <span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">CustomOrdering</span></span><br><span class=\"hljs-class\"></span><br><span class=\"hljs-class\"><span class=\"hljs-title\">scala&gt;</span> <span class=\"hljs-title\">val</span> <span class=\"hljs-title\">list=List</span>(<span class=\"hljs-params\">(1,&quot;hadoop&quot;</span>),(<span class=\"hljs-params\">1,&quot;xiaokangxxs&quot;</span>),(<span class=\"hljs-params\">1,&quot;azkaban&quot;</span>),(<span class=\"hljs-params\">1,&quot;hive&quot;</span>),(<span class=\"hljs-params\">1,&quot;spark&quot;</span>))</span><br><span class=\"hljs-class\"><span class=\"hljs-title\">list</span></span>: <span class=\"hljs-type\">List</span>[(<span class=\"hljs-type\">Int</span>, <span class=\"hljs-type\">String</span>)] = <span class=\"hljs-type\">List</span>((<span class=\"hljs-number\">1</span>,hadoop), (<span class=\"hljs-number\">1</span>,xiaokangxxs), (<span class=\"hljs-number\">1</span>,azkaban), (<span class=\"hljs-number\">1</span>,hive), (<span class=\"hljs-number\">1</span>,spark))<br><br>scala&gt; <span class=\"hljs-keyword\">implicit</span> <span class=\"hljs-keyword\">val</span> implicitOrdering=<span class=\"hljs-keyword\">new</span> <span class=\"hljs-type\">CustomOrdering</span><br>implicitOrdering: <span class=\"hljs-type\">CustomOrdering</span> = <span class=\"hljs-type\">CustomOrdering</span>@<span class=\"hljs-number\">3102</span>c986<br><br>scala&gt; sc.parallelize(list).takeOrdered(<span class=\"hljs-number\">5</span>)<br>res20: <span class=\"hljs-type\">Array</span>[(<span class=\"hljs-type\">Int</span>, <span class=\"hljs-type\">String</span>)] = <span class=\"hljs-type\">Array</span>((<span class=\"hljs-number\">1</span>,hive), (<span class=\"hljs-number\">1</span>,spark), (<span class=\"hljs-number\">1</span>,hadoop), (<span class=\"hljs-number\">1</span>,azkaban), (<span class=\"hljs-number\">1</span>,xiaokangxxs))<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"2-6-countByKey\"><a href=\"#2-6-countByKey\" class=\"headerlink\" title=\"2.6 countByKey\"></a>2.6 countByKey</h3><p>计算每个键出现的次数：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>((<span class=\"hljs-string\">&quot;hadoop&quot;</span>,<span class=\"hljs-number\">10</span>),(<span class=\"hljs-string\">&quot;hadoop&quot;</span>,<span class=\"hljs-number\">10</span>),(<span class=\"hljs-string\">&quot;xiaokangxxs&quot;</span>,<span class=\"hljs-number\">3</span>),(<span class=\"hljs-string\">&quot;xiaokang&quot;</span>,<span class=\"hljs-number\">3</span>),(<span class=\"hljs-string\">&quot;azkaban&quot;</span>,<span class=\"hljs-number\">1</span>))<br>scala&gt; sc.parallelize(list).countByKey()<br>res23: scala.collection.<span class=\"hljs-type\">Map</span>[<span class=\"hljs-type\">String</span>,<span class=\"hljs-type\">Long</span>] = <span class=\"hljs-type\">Map</span>(xiaokangxxs -&gt; <span class=\"hljs-number\">1</span>, hadoop -&gt; <span class=\"hljs-number\">2</span>, xiaokang -&gt; <span class=\"hljs-number\">1</span>, azkaban -&gt; <span class=\"hljs-number\">1</span>)<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"2-7-saveAsTextFile\"><a href=\"#2-7-saveAsTextFile\" class=\"headerlink\" title=\"2.7 saveAsTextFile\"></a>2.7 saveAsTextFile</h3><p>将RDD中的元素以文本文件的形式写入本地文件系统、HDFS或其它Hadoop支持的文件系统中。Spark将对每个元素调用toString方法，将元素转换为文本文件中的一行记录。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>((<span class=\"hljs-string\">&quot;hadoop&quot;</span>,<span class=\"hljs-number\">10</span>),(<span class=\"hljs-string\">&quot;hadoop&quot;</span>,<span class=\"hljs-number\">10</span>),(<span class=\"hljs-string\">&quot;xiaokangxxs&quot;</span>, <span class=\"hljs-number\">3</span>), (<span class=\"hljs-string\">&quot;spark&quot;</span>, <span class=\"hljs-number\">3</span>), (<span class=\"hljs-string\">&quot;azkaban&quot;</span>, <span class=\"hljs-number\">1</span>))<br>scala&gt; sc.parallelize(list).saveAsTextFile(<span class=\"hljs-string\">&quot;file:///home/xiaokang/file_out&quot;</span>)<br><br>[xiaokang<span class=\"hljs-meta\">@xk</span>1181259634 ~/file_out <span class=\"hljs-number\">18</span>:<span class=\"hljs-number\">03</span>:<span class=\"hljs-number\">23</span>]$cat /home/xiaokang/file_out/part<span class=\"hljs-number\">-00000</span><br>(hadoop,<span class=\"hljs-number\">10</span>)<br>(hadoop,<span class=\"hljs-number\">10</span>)<br>(xiaokangxxs,<span class=\"hljs-number\">3</span>)<br>(spark,<span class=\"hljs-number\">3</span>)<br>(azkaban,<span class=\"hljs-number\">1</span>)<br></code></pre></td></tr></table></figure>\n","more":"<h2 id=\"一、Transformation\"><a href=\"#一、Transformation\" class=\"headerlink\" title=\"一、Transformation\"></a>一、Transformation</h2><p>Spark常用的Transformation算子如下表：</p>\n<table>\n<thead>\n<tr>\n<th>Transformation 算子</th>\n<th>Meaning（含义）</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>map</strong>(<em>func</em>)</td>\n<td>对原RDD中每个元素运用<em>func</em>函数，并生成新的RDD</td>\n</tr>\n<tr>\n<td><strong>filter</strong>(<em>func</em>)</td>\n<td>对原RDD中每个元素使用<em>func</em>函数进行过滤，并生成新的RDD</td>\n</tr>\n<tr>\n<td><strong>flatMap</strong>(<em>func</em>)</td>\n<td>与map类似，但是每一个输入的item被映射成0个或多个输出的items（ <em>func</em>返回类型需要为Seq）。</td>\n</tr>\n<tr>\n<td><strong>mapPartitions</strong>(<em>func</em>)</td>\n<td>与map类似，但函数单独在RDD的每个分区上运行， <em>func</em>函数的类型为*Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt;*，其中T是RDD的类型，即RDD[T]</td>\n</tr>\n<tr>\n<td><strong>mapPartitionsWithIndex</strong>(<em>func</em>)</td>\n<td>与mapPartitions类似，但<em>func</em>类型为 (Int,Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt;，其中第一个参数为分区索引</td>\n</tr>\n<tr>\n<td><strong>sample</strong>(<em>withReplacement</em>, <em>fraction</em>, <em>seed</em>)</td>\n<td>数据采样，有三个可选参数：设置是否放回（withReplacement）、采样的百分比（<em>fraction</em>）、随机数生成器的种子（seed）。</td>\n</tr>\n<tr>\n<td><strong>union</strong>(<em>otherDataset</em>)</td>\n<td>合并两个RDD</td>\n</tr>\n<tr>\n<td><strong>intersection</strong>(<em>otherDataset</em>)</td>\n<td>求两个RDD的交集</td>\n</tr>\n<tr>\n<td><strong>distinct</strong>([<em>numTasks</em>]))</td>\n<td>去重</td>\n</tr>\n<tr>\n<td><strong>groupByKey</strong>([<em>numTasks</em>])</td>\n<td>按照key值进行分区，即在一个 (K, V) 对的dataset上调用时，返回一个 (K, Iterable&lt;V&gt;) <strong>Note:</strong> 如果分组是为了在每一个key上执行聚合操作（例如，sum 或 average)，此时使用 <code>reduceByKey</code> 或<code>aggregateByKey</code> 性能会更好<strong>Note:</strong> 默认情况下，并行度取决于父 RDD 的分区数。可以传入 <code>numTasks</code> 参数进行修改。</td>\n</tr>\n<tr>\n<td><strong>reduceByKey</strong>(<em>func</em>, [<em>numTasks</em>])</td>\n<td>按照key值进行分组，并对分组后的数据执行归约操作（于groupByKey相比，reduceByKey会有提前combine操作）。</td>\n</tr>\n<tr>\n<td><strong>aggregateByKey</strong>(<em>zeroValue</em>,<em>numPartitions</em>)(<em>seqOp</em>, <em>combOp</em>, [<em>numTasks</em>])</td>\n<td>当调用（K，V）对的数据集时，返回（K，U）对的数据集，其中使用给定的组合函数和zeroValue聚合每个键的值。与groupByKey类似，reduce任务的数量可通过第二个参数进行配置。</td>\n</tr>\n<tr>\n<td><strong>sortByKey</strong>([<em>ascending</em>], [<em>numTasks</em>])</td>\n<td>按照key进行排序，其中的key需要实现Ordered特质，即可比较</td>\n</tr>\n<tr>\n<td><strong>join</strong>(<em>otherDataset</em>, [<em>numTasks</em>])</td>\n<td>在一个 (K, V) 和 (K, W) 类型的dataset上调用时，返回一个 (K, (V, W)) pairs 的dataset，等价于内连接操作。如果想要执行外连接，可以使用 <code>leftOuterJoin</code>, <code>rightOuterJoin</code> 和 <code>fullOuterJoin</code> 等算子。</td>\n</tr>\n<tr>\n<td><strong>cogroup</strong>(<em>otherDataset</em>, [<em>numTasks</em>])</td>\n<td>在一个 (K, V) 对的dataset上调用时，返回一个 (K, (Iterable&lt;V&gt;, Iterable&lt;W&gt;)) tuples的dataset。</td>\n</tr>\n<tr>\n<td><strong>cartesian</strong>(<em>otherDataset</em>)</td>\n<td>在一个T和U类型的dataset上调用时，返回一个 (T, U) 类型的dataset（即笛卡尔积）。</td>\n</tr>\n<tr>\n<td><strong>coalesce</strong>(<em>numPartitions</em>)</td>\n<td>将RDD中的分区数减少为numPartitions。</td>\n</tr>\n<tr>\n<td><strong>repartition</strong>(<em>numPartitions</em>)</td>\n<td>随机重新调整RDD中的数据以创建更多或更少的分区，并在它们之间进行平衡。</td>\n</tr>\n<tr>\n<td><strong>repartitionAndSortWithinPartitions</strong>(<em>partitioner</em>)</td>\n<td>根据给定的partitioner（分区器）对RDD进行重新分区，并对分区中的数据按照key值进行排序。这比调用 <code>repartition</code> 然后再sorting（排序）效率更高，因为它可以将排序过程推送到shuffle操作所在的机器。</td>\n</tr>\n</tbody></table>\n<p>下面给出部分算子的基本使用示例：</p>\n<h3 id=\"1-1-map\"><a href=\"#1-1-map\" class=\"headerlink\" title=\"1.1 map\"></a>1.1 map</h3><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">4</span>)<br>scala&gt; sc.parallelize(list).map(_ * <span class=\"hljs-number\">2</span>).foreach(println)<br><span class=\"hljs-number\">2</span><br><span class=\"hljs-number\">2</span><br><span class=\"hljs-number\">4</span><br><span class=\"hljs-number\">8</span><br></code></pre></td></tr></table></figure>\n\n<h3 id=\"1-2-filter\"><a href=\"#1-2-filter\" class=\"headerlink\" title=\"1.2 filter\"></a>1.2 filter</h3><figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">4</span>)<br>scala&gt; sc.parallelize(list).filter(_ &gt;= <span class=\"hljs-number\">2</span>).foreach(println)<br><span class=\"hljs-number\">2</span><br><span class=\"hljs-number\">4</span><br></code></pre></td></tr></table></figure>\n\n<h3 id=\"1-3-flatMap\"><a href=\"#1-3-flatMap\" class=\"headerlink\" title=\"1.3 flatMap\"></a>1.3 flatMap</h3><p><code>flatMap(func)</code> 与 <code>map</code> 类似，但每一个输入的item会被映射成0个或多个输出的items（<em>func</em>返回类型需要为<code>Seq</code>）。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>(<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>), <span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">4</span>), <span class=\"hljs-type\">List</span>(), <span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">4</span>,<span class=\"hljs-number\">6</span>,<span class=\"hljs-number\">8</span>))<br>scala&gt; sc.parallelize(list).flatMap(_.toList).foreach(println)<br><span class=\"hljs-number\">1</span><br><span class=\"hljs-number\">2</span><br><span class=\"hljs-number\">4</span><br><span class=\"hljs-number\">4</span><br><span class=\"hljs-number\">6</span><br><span class=\"hljs-number\">8</span><br></code></pre></td></tr></table></figure>\n\n<p>flatMap在日志分析中使用概率非常高，这里进行一下演示：拆分输入的每行数据为单个单词，并赋值为1，代表出现一次，之后按照单词分组并统计其出现总次数，代码如下：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> lines=<span class=\"hljs-type\">List</span>(<span class=\"hljs-string\">&quot;spark flume xiaokang spark scala&quot;</span>,<span class=\"hljs-string\">&quot;hadoop scala xiaokangxxs xiaokang flume hive&quot;</span>)<br>scala&gt; sc.parallelize(lines).flatMap(_.split(<span class=\"hljs-string\">&quot; &quot;</span>)).map((_,<span class=\"hljs-number\">1</span>)).reduceByKey(_+_).foreach(println)<br>(scala,<span class=\"hljs-number\">2</span>)<br>(xiaokangxxs,<span class=\"hljs-number\">1</span>)<br>(spark,<span class=\"hljs-number\">2</span>)<br>(hive,<span class=\"hljs-number\">1</span>)<br>(hadoop,<span class=\"hljs-number\">1</span>)<br>(flume,<span class=\"hljs-number\">2</span>)<br>(xiaokang,<span class=\"hljs-number\">2</span>)<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"1-4-mapPartitions\"><a href=\"#1-4-mapPartitions\" class=\"headerlink\" title=\"1.4 mapPartitions\"></a>1.4 mapPartitions</h3><p>与map类似，但函数单独在RDD的每个分区上运行， <em>func</em>函数的类型为<code>Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt;</code> (其中T是RDD的类型)，即输入和输出都必须是可迭代类型。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">import</span> scala.collection.mutable.<span class=\"hljs-type\">ListBuffer</span><br><span class=\"hljs-keyword\">import</span> scala.collection.mutable.<span class=\"hljs-type\">ListBuffer</span><br>scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">4</span>,<span class=\"hljs-number\">6</span>,<span class=\"hljs-number\">8</span>)<br>scala&gt; sc.parallelize(list, <span class=\"hljs-number\">3</span>).mapPartitions(iterator =&gt; &#123;<br>     |   <span class=\"hljs-keyword\">val</span> buffer = <span class=\"hljs-keyword\">new</span> <span class=\"hljs-type\">ListBuffer</span>[<span class=\"hljs-type\">Int</span>]<br>     |   <span class=\"hljs-keyword\">while</span> (iterator.hasNext) &#123;<br>     |      buffer.append(iterator.next()*<span class=\"hljs-number\">3</span>)<br>     |   &#125;<br>     |   buffer.toIterator&#125;).foreach(println)<br><span class=\"hljs-number\">3</span><br><span class=\"hljs-number\">3</span><br><span class=\"hljs-number\">6</span><br><span class=\"hljs-number\">12</span><br><span class=\"hljs-number\">18</span><br><span class=\"hljs-number\">24</span><br></code></pre></td></tr></table></figure>\n\n<h3 id=\"1-5-mapPartitionsWithIndex\"><a href=\"#1-5-mapPartitionsWithIndex\" class=\"headerlink\" title=\"1.5 mapPartitionsWithIndex\"></a>1.5 mapPartitionsWithIndex</h3><p>  与mapPartitions类似，但<em>func</em>类型为 <code>(Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt;</code> ，其中第一个参数为分区索引。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">import</span> scala.collection.mutable.<span class=\"hljs-type\">ListBuffer</span><br><span class=\"hljs-keyword\">import</span> scala.collection.mutable.<span class=\"hljs-type\">ListBuffer</span><br>scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">4</span>,<span class=\"hljs-number\">6</span>,<span class=\"hljs-number\">8</span>)<br>scala&gt; sc.parallelize(list, <span class=\"hljs-number\">3</span>).mapPartitionsWithIndex((index,iterator) =&gt; &#123;<br>     |   <span class=\"hljs-keyword\">val</span> buffer = <span class=\"hljs-keyword\">new</span> <span class=\"hljs-type\">ListBuffer</span>[<span class=\"hljs-type\">String</span>]<br>     |   <span class=\"hljs-keyword\">while</span> (iterator.hasNext) &#123;<br>     |      buffer.append(index+<span class=\"hljs-string\">&quot;---&quot;</span>+iterator.next()*<span class=\"hljs-number\">3</span>)<br>     |   &#125;<br>     |   buffer.toIterator&#125;).foreach(println)<br><span class=\"hljs-number\">0</span>--<span class=\"hljs-number\">-3</span><br><span class=\"hljs-number\">0</span>--<span class=\"hljs-number\">-3</span><br><span class=\"hljs-number\">1</span>--<span class=\"hljs-number\">-6</span><br><span class=\"hljs-number\">1</span>--<span class=\"hljs-number\">-12</span><br><span class=\"hljs-number\">2</span>--<span class=\"hljs-number\">-18</span><br><span class=\"hljs-number\">2</span>--<span class=\"hljs-number\">-24</span><br></code></pre></td></tr></table></figure>\n\n<h3 id=\"1-6-sample\"><a href=\"#1-6-sample\" class=\"headerlink\" title=\"1.6 sample\"></a>1.6 sample</h3><p>数据采样。有三个可选参数：设置是否放回 (withReplacement)、采样的百分比 (fraction)、随机数生成器的种子 (seed) ：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">4</span>,<span class=\"hljs-number\">6</span>,<span class=\"hljs-number\">8</span>)<br>scala&gt; sc.parallelize(list).sample(withReplacement=<span class=\"hljs-literal\">false</span>,fraction=<span class=\"hljs-number\">0.5</span>).foreach(println)<br><span class=\"hljs-number\">1</span><br><span class=\"hljs-number\">1</span><br><span class=\"hljs-number\">4</span><br><span class=\"hljs-number\">6</span><br>scala&gt; sc.parallelize(list).sample(withReplacement=<span class=\"hljs-literal\">false</span>,fraction=<span class=\"hljs-number\">0.5</span>).foreach(println)<br><span class=\"hljs-number\">1</span><br><span class=\"hljs-number\">8</span><br>scala&gt; sc.parallelize(list).sample(withReplacement=<span class=\"hljs-literal\">false</span>,fraction=<span class=\"hljs-number\">0.5</span>).foreach(println)<br><span class=\"hljs-number\">1</span><br>scala&gt; sc.parallelize(list).sample(withReplacement=<span class=\"hljs-literal\">false</span>,fraction=<span class=\"hljs-number\">0.5</span>).foreach(println)<br><span class=\"hljs-number\">1</span><br><span class=\"hljs-number\">4</span><br><span class=\"hljs-number\">6</span><br></code></pre></td></tr></table></figure>\n\n<h3 id=\"1-7-union\"><a href=\"#1-7-union\" class=\"headerlink\" title=\"1.7 union\"></a>1.7 union</h3><p>合并两个RDD：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list1=<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">3</span>)<br>scala&gt; <span class=\"hljs-keyword\">val</span> list2=<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">6</span>,<span class=\"hljs-number\">8</span>,<span class=\"hljs-number\">10</span>)<br>scala&gt; sc.parallelize(list1).union(sc.parallelize(list2)).foreach(println)<br><span class=\"hljs-number\">1</span><br><span class=\"hljs-number\">2</span><br><span class=\"hljs-number\">3</span><br><span class=\"hljs-number\">6</span><br><span class=\"hljs-number\">8</span><br><span class=\"hljs-number\">10</span><br></code></pre></td></tr></table></figure>\n\n<h3 id=\"1-8-intersection\"><a href=\"#1-8-intersection\" class=\"headerlink\" title=\"1.8 intersection\"></a>1.8 intersection</h3><p>求两个 RDD 的交集：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list1=<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">3</span>,<span class=\"hljs-number\">6</span>)<br>scala&gt; <span class=\"hljs-keyword\">val</span> list2=<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">5</span>,<span class=\"hljs-number\">6</span>,<span class=\"hljs-number\">9</span>,<span class=\"hljs-number\">11</span>)<br>scala&gt; sc.parallelize(list1).intersection(sc.parallelize(list2)).foreach(println)<br><span class=\"hljs-number\">6</span><br></code></pre></td></tr></table></figure>\n\n<h3 id=\"1-9-distinct\"><a href=\"#1-9-distinct\" class=\"headerlink\" title=\"1.9 distinct\"></a>1.9 distinct</h3><p>去重：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">4</span>,<span class=\"hljs-number\">6</span>,<span class=\"hljs-number\">6</span>,<span class=\"hljs-number\">8</span>)<br>scala&gt; sc.parallelize(list).distinct().foreach(println)<br><span class=\"hljs-number\">4</span><br><span class=\"hljs-number\">1</span><br><span class=\"hljs-number\">6</span><br><span class=\"hljs-number\">8</span><br><span class=\"hljs-number\">2</span><br></code></pre></td></tr></table></figure>\n\n<h3 id=\"1-10-groupByKey\"><a href=\"#1-10-groupByKey\" class=\"headerlink\" title=\"1.10 groupByKey\"></a>1.10 groupByKey</h3><p>按照键进行分组：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>((<span class=\"hljs-string\">&quot;hadoop&quot;</span>, <span class=\"hljs-number\">2</span>), (<span class=\"hljs-string\">&quot;spark&quot;</span>, <span class=\"hljs-number\">3</span>), (<span class=\"hljs-string\">&quot;xiaokang&quot;</span>, <span class=\"hljs-number\">5</span>), (<span class=\"hljs-string\">&quot;spark&quot;</span>, <span class=\"hljs-number\">6</span>), (<span class=\"hljs-string\">&quot;hadoop&quot;</span>, <span class=\"hljs-number\">2</span>))<br>scala&gt; sc.parallelize(list).groupByKey().map(x =&gt; (x._1, x._2.toList)).foreach(println)<br>(spark,<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">3</span>, <span class=\"hljs-number\">6</span>))<br>(hadoop,<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">2</span>))<br>(xiaokang,<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">5</span>))<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"1-11-reduceByKey\"><a href=\"#1-11-reduceByKey\" class=\"headerlink\" title=\"1.11 reduceByKey\"></a>1.11 reduceByKey</h3><p>按照键进行归约操作：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>((<span class=\"hljs-string\">&quot;hadoop&quot;</span>, <span class=\"hljs-number\">2</span>), (<span class=\"hljs-string\">&quot;spark&quot;</span>, <span class=\"hljs-number\">3</span>), (<span class=\"hljs-string\">&quot;xiaokang&quot;</span>, <span class=\"hljs-number\">5</span>), (<span class=\"hljs-string\">&quot;spark&quot;</span>, <span class=\"hljs-number\">6</span>), (<span class=\"hljs-string\">&quot;hadoop&quot;</span>, <span class=\"hljs-number\">2</span>))<br>scala&gt; sc.parallelize(list).reduceByKey(_+_).foreach(println)<br>(spark,<span class=\"hljs-number\">9</span>)<br>(hadoop,<span class=\"hljs-number\">4</span>)<br>(xiaokang,<span class=\"hljs-number\">5</span>)<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"1-12-sortBy-amp-sortByKey\"><a href=\"#1-12-sortBy-amp-sortByKey\" class=\"headerlink\" title=\"1.12 sortBy &amp; sortByKey\"></a>1.12 sortBy &amp; sortByKey</h3><p>按照键进行排序：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>((<span class=\"hljs-number\">100</span>, <span class=\"hljs-string\">&quot;hadoop&quot;</span>), (<span class=\"hljs-number\">90</span>, <span class=\"hljs-string\">&quot;spark&quot;</span>), (<span class=\"hljs-number\">120</span>, <span class=\"hljs-string\">&quot;xiaokangxxs&quot;</span>))<br>scala&gt; sc.parallelize(list).sortByKey(ascending=<span class=\"hljs-literal\">false</span>).foreach(println)<br>(<span class=\"hljs-number\">120</span>,xiaokangxxs)<br>(<span class=\"hljs-number\">100</span>,hadoop)<br>(<span class=\"hljs-number\">90</span>,spark)<br></code></pre></td></tr></table></figure>\n\n<p>按照指定元素进行排序：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>((<span class=\"hljs-string\">&quot;hadoop&quot;</span>,<span class=\"hljs-number\">100</span>), (<span class=\"hljs-string\">&quot;spark&quot;</span>,<span class=\"hljs-number\">90</span>), (<span class=\"hljs-string\">&quot;xiaokangxxs&quot;</span>,<span class=\"hljs-number\">120</span>))<br>scala&gt; sc.parallelize(list).sortBy(x=&gt;x._2,ascending=<span class=\"hljs-literal\">false</span>).foreach(println)<br>(xiaokangxxs,<span class=\"hljs-number\">120</span>)<br>(hadoop,<span class=\"hljs-number\">100</span>)<br>(spark,<span class=\"hljs-number\">90</span>)<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"1-13-join\"><a href=\"#1-13-join\" class=\"headerlink\" title=\"1.13 join\"></a>1.13 join</h3><p>在一个 (K, V) 和 (K, W) 类型的Dataset上调用时，返回一个 (K, (V, W)) 的Dataset，等价于内连接操作。如果想要执行外连接，可以使用 <code>leftOuterJoin</code>, <code>rightOuterJoin</code> 和 <code>fullOuterJoin</code> 等算子。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list1=<span class=\"hljs-type\">List</span>((<span class=\"hljs-number\">1</span>,<span class=\"hljs-string\">&quot;student01&quot;</span>),(<span class=\"hljs-number\">2</span>,<span class=\"hljs-string\">&quot;student02&quot;</span>),(<span class=\"hljs-number\">3</span>,<span class=\"hljs-string\">&quot;student03&quot;</span>))<br>scala&gt; <span class=\"hljs-keyword\">val</span> list2=<span class=\"hljs-type\">List</span>((<span class=\"hljs-number\">1</span>,<span class=\"hljs-string\">&quot;teacher01&quot;</span>),(<span class=\"hljs-number\">2</span>,<span class=\"hljs-string\">&quot;teacher02&quot;</span>),(<span class=\"hljs-number\">3</span>,<span class=\"hljs-string\">&quot;teacher03&quot;</span>))<br>scala&gt; sc.parallelize(list1).join(sc.parallelize(list2)).foreach(println)<br>(<span class=\"hljs-number\">1</span>,(student01,teacher01))<br>(<span class=\"hljs-number\">3</span>,(student03,teacher03))<br>(<span class=\"hljs-number\">2</span>,(student02,teacher02))<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"1-14-cartesian\"><a href=\"#1-14-cartesian\" class=\"headerlink\" title=\"1.14 cartesian\"></a>1.14 cartesian</h3><p>计算笛卡尔积：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list1=<span class=\"hljs-type\">List</span>(<span class=\"hljs-string\">&quot;xiaokang&quot;</span>, <span class=\"hljs-string\">&quot;spark&quot;</span>, <span class=\"hljs-string\">&quot;flink&quot;</span>)<br>scala&gt; <span class=\"hljs-keyword\">val</span> list2=<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">3</span>)<br>scala&gt; sc.parallelize(list1).cartesian(sc.parallelize(list2)).foreach(println)<br>(xiaokang,<span class=\"hljs-number\">1</span>)<br>(xiaokang,<span class=\"hljs-number\">2</span>)<br>(xiaokang,<span class=\"hljs-number\">3</span>)<br>(spark,<span class=\"hljs-number\">1</span>)<br>(spark,<span class=\"hljs-number\">2</span>)<br>(spark,<span class=\"hljs-number\">3</span>)<br>(flink,<span class=\"hljs-number\">1</span>)<br>(flink,<span class=\"hljs-number\">2</span>)<br>(flink,<span class=\"hljs-number\">3</span>)<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"1-15-aggregateByKey\"><a href=\"#1-15-aggregateByKey\" class=\"headerlink\" title=\"1.15 aggregateByKey\"></a>1.15 aggregateByKey</h3><p>当调用（K，V）对的数据集时，返回（K，U）对的数据集，其中使用给定的组合函数和zeroValue聚合每个键的值。与<code>groupByKey</code>类似，reduce任务的数量可通过第二个参数<code>numPartitions</code>进行配置。示例如下：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>((<span class=\"hljs-string\">&quot;hadoop&quot;</span>,<span class=\"hljs-number\">3</span>), (<span class=\"hljs-string\">&quot;hadoop&quot;</span>,<span class=\"hljs-number\">2</span>), (<span class=\"hljs-string\">&quot;spark&quot;</span>,<span class=\"hljs-number\">4</span>), (<span class=\"hljs-string\">&quot;spark&quot;</span>,<span class=\"hljs-number\">3</span>), (<span class=\"hljs-string\">&quot;storm&quot;</span>,<span class=\"hljs-number\">6</span>), (<span class=\"hljs-string\">&quot;storm&quot;</span>,<span class=\"hljs-number\">8</span>))<br>scala&gt; sc.parallelize(list,numSlices = <span class=\"hljs-number\">2</span>).aggregateByKey(zeroValue = <span class=\"hljs-number\">0</span>,numPartitions = <span class=\"hljs-number\">3</span>)(<br>     |  seqOp=math.max(_, _),<br>     |  combOp=_ + _<br>     | ).collect.foreach(println)<br>(hadoop,<span class=\"hljs-number\">3</span>)<br>(storm,<span class=\"hljs-number\">8</span>)<br>(spark,<span class=\"hljs-number\">7</span>)<br></code></pre></td></tr></table></figure>\n\n<p>这里使用了<code>numSlices = 2</code>指定aggregateByKey父操作parallelize的分区数量为2，其执行流程如下：</p>\n<div align=\"center\"> <img src=\"https://gitee.com/heibaiying/BigData-Notes/raw/master/pictures/spark-aggregateByKey.png\"/> </div>\n基于同样的执行流程，如果`numSlices=1`，则意味着只有输入一个分区，则其最后一步combOp相当于是无效的，执行结果为：\n\n<figure class=\"highlight properties\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs properties\"><span class=\"hljs-attr\">(hadoop,3)</span><br><span class=\"hljs-attr\">(storm,8)</span><br><span class=\"hljs-attr\">(spark,4)</span><br></code></pre></td></tr></table></figure>\n\n<p>同样的，如果每个单词对一个分区，即<code>numSlices=6</code>，此时相当于求和操作，执行结果为：</p>\n<figure class=\"highlight properties\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs properties\"><span class=\"hljs-attr\">(hadoop,5)</span><br><span class=\"hljs-attr\">(storm,14)</span><br><span class=\"hljs-attr\">(spark,7)</span><br></code></pre></td></tr></table></figure>\n\n<p><code>aggregateByKey(zeroValue=0,numPartitions=3)</code>的第二个参数<code>numPartitions</code> 决定的是输出 RDD 的分区数量，想要验证这个问题，可以对上面代码进行改写，使用<code>getNumPartitions</code>方法获取分区数量：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; sc.parallelize(list,numSlices=<span class=\"hljs-number\">6</span>).aggregateByKey(zeroValue=<span class=\"hljs-number\">0</span>,numPartitions=<span class=\"hljs-number\">3</span>)(<br>     |   seqOp = math.max(_, _),<br>     |   combOp = _ + _<br>     | ).getNumPartitions<br>res16: <span class=\"hljs-type\">Int</span> = <span class=\"hljs-number\">3</span><br></code></pre></td></tr></table></figure>\n\n<h2 id=\"二、Action\"><a href=\"#二、Action\" class=\"headerlink\" title=\"二、Action\"></a>二、Action</h2><p>Spark常用的Action算子如下：</p>\n<table>\n<thead>\n<tr>\n<th>Action（动作）</th>\n<th>Meaning（含义）</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>reduce</strong>(<em>func</em>)</td>\n<td>使用函数<em>func</em>执行归约操作</td>\n</tr>\n<tr>\n<td><strong>collect</strong>()</td>\n<td>以一个array数组的形式返回dataset的所有元素，适用于小结果集。</td>\n</tr>\n<tr>\n<td><strong>count</strong>()</td>\n<td>返回RDD中元素的个数。</td>\n</tr>\n<tr>\n<td><strong>first</strong>()</td>\n<td>返回RDD中的第一个元素，不排序。</td>\n</tr>\n<tr>\n<td><strong>take</strong>(<em>n</em>)</td>\n<td>将RDD中的前<em>n</em>个元素作为一个array数组返回。</td>\n</tr>\n<tr>\n<td><strong>takeSample</strong>(<em>withReplacement</em>, <em>num</em>, [<em>seed</em>])</td>\n<td>对一个RDD进行随机抽样</td>\n</tr>\n<tr>\n<td><strong>takeOrdered</strong>(<em>n</em>, <em>[ordering]</em>)</td>\n<td>按自然顺序（natural order）或自定义比较器（custom comparator）排序后返回前<em>n</em>个元素。只适用于小结果集，因为所有数据都会被加载到驱动程序的内存中进行排序。</td>\n</tr>\n<tr>\n<td><strong>saveAsTextFile</strong>(<em>path</em>)</td>\n<td>将RDD中的元素以文本文件的形式写入本地文件系统、HDFS或其它 Hadoop支持的文件系统中。Spark将对每个元素调用toString方法，将元素转换为文本文件中的一行记录。</td>\n</tr>\n<tr>\n<td><strong>saveAsSequenceFile</strong>(<em>path</em>)</td>\n<td>将RDD中的元素以Hadoop SequenceFile的形式写入到本地文件系统、HDFS或其它Hadoop支持的文件系统中。该操作要求RDD中的元素需要实现Hadoop的Writable接口。对于Scala语言而言，它可以将Spark中的基本数据类型自动隐式转换为对应Writable类型。(目前仅支持Java and Scala)</td>\n</tr>\n<tr>\n<td><strong>saveAsObjectFile</strong>(<em>path</em>)</td>\n<td>使用Java序列化后存储，可以使用<code>SparkContext.objectFile()</code>进行加载。(目前仅支持Java and Scala)</td>\n</tr>\n<tr>\n<td><strong>countByKey</strong>()</td>\n<td>计算每个键出现的次数。</td>\n</tr>\n<tr>\n<td><strong>foreach</strong>(<em>func</em>)</td>\n<td>遍历RDD中每个元素，并对其执行<em>fun</em>函数</td>\n</tr>\n<tr>\n<td><strong>foreachPartition</strong>(<em>func</em>)</td>\n<td>与foreach类似，但函数单独在RDD的每个分区上运行，运用<em>func</em>函数，并生成新的RDD。</td>\n</tr>\n</tbody></table>\n<p>下面给出部分算子的基本使用示例：</p>\n<h3 id=\"2-1-take\"><a href=\"#2-1-take\" class=\"headerlink\" title=\"2.1 take\"></a>2.1 take</h3><p>将RDD中的前<em>n</em>个元素作为一个array数组返回：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">4</span>,<span class=\"hljs-number\">6</span>,<span class=\"hljs-number\">8</span>,<span class=\"hljs-number\">11</span>)<br>scala&gt; sc.parallelize(list).take(<span class=\"hljs-number\">3</span>)<br>res21: <span class=\"hljs-type\">Array</span>[<span class=\"hljs-type\">Int</span>] = <span class=\"hljs-type\">Array</span>(<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>)<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"2-3-first\"><a href=\"#2-3-first\" class=\"headerlink\" title=\"2.3 first\"></a>2.3 first</h3><p>返回RDD中的第一个元素，不排序：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">4</span>,<span class=\"hljs-number\">6</span>,<span class=\"hljs-number\">8</span>,<span class=\"hljs-number\">11</span>)<br>scala&gt; sc.parallelize(list).first<br>res22: <span class=\"hljs-type\">Int</span> = <span class=\"hljs-number\">1</span><br></code></pre></td></tr></table></figure>\n\n<h3 id=\"2-4-reduce\"><a href=\"#2-4-reduce\" class=\"headerlink\" title=\"2.4 reduce\"></a>2.4 reduce</h3><p>使用函数<em>func</em>执行归约操作：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>(<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">3</span>,<span class=\"hljs-number\">4</span>,<span class=\"hljs-number\">5</span>,<span class=\"hljs-number\">6</span>,<span class=\"hljs-number\">7</span>,<span class=\"hljs-number\">8</span>,<span class=\"hljs-number\">9</span>,<span class=\"hljs-number\">10</span>)<br>scala&gt; sc.parallelize(list).reduce(_+_)<br>res18: <span class=\"hljs-type\">Int</span> = <span class=\"hljs-number\">55</span><br></code></pre></td></tr></table></figure>\n\n<h3 id=\"2-5-takeOrdered\"><a href=\"#2-5-takeOrdered\" class=\"headerlink\" title=\"2.5 takeOrdered\"></a>2.5 takeOrdered</h3><p>按自然顺序（natural order）或自定义比较器（custom comparator）排序后返回前 <em>n</em> 个元素。需要注意的是 <code>takeOrdered</code> 使用隐式参数进行隐式转换，以下为其源码。所以在使用自定义排序时，需要继承 <code>Ordering[T]</code> 实现自定义比较器，然后将其作为隐式参数引入。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\"><span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">takeOrdered</span></span>(num: <span class=\"hljs-type\">Int</span>)(<span class=\"hljs-keyword\">implicit</span> ord: <span class=\"hljs-type\">Ordering</span>[<span class=\"hljs-type\">T</span>]): <span class=\"hljs-type\">Array</span>[<span class=\"hljs-type\">T</span>] = withScope &#123;<br>  .........<br>&#125;<br></code></pre></td></tr></table></figure>\n\n<p>自定义规则排序：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\"><span class=\"hljs-comment\">// 继承 Ordering[T],实现自定义比较器，按照 value 值的长度进行排序</span><br><span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">CustomOrdering</span> <span class=\"hljs-keyword\">extends</span> <span class=\"hljs-title\">Ordering</span>[(<span class=\"hljs-type\">Int</span>, <span class=\"hljs-type\">String</span>)] </span>&#123;<br>    <span class=\"hljs-keyword\">override</span> <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">compare</span></span>(x: (<span class=\"hljs-type\">Int</span>, <span class=\"hljs-type\">String</span>), y: (<span class=\"hljs-type\">Int</span>, <span class=\"hljs-type\">String</span>)): <span class=\"hljs-type\">Int</span><br>    = <span class=\"hljs-keyword\">if</span> (x._2.length &gt; y._2.length) <span class=\"hljs-number\">1</span> <span class=\"hljs-keyword\">else</span> <span class=\"hljs-number\">-1</span><br>&#125;<br></code></pre></td></tr></table></figure>\n\n<p>具体使用：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">CustomOrdering</span> <span class=\"hljs-keyword\">extends</span> <span class=\"hljs-title\">Ordering</span>[(<span class=\"hljs-type\">Int</span>, <span class=\"hljs-type\">String</span>)] </span>&#123;<br>     |     <span class=\"hljs-keyword\">override</span> <span class=\"hljs-function\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title\">compare</span></span>(x: (<span class=\"hljs-type\">Int</span>, <span class=\"hljs-type\">String</span>), y: (<span class=\"hljs-type\">Int</span>, <span class=\"hljs-type\">String</span>)): <span class=\"hljs-type\">Int</span><br>     |     = <span class=\"hljs-keyword\">if</span> (x._2.length &gt; y._2.length) <span class=\"hljs-number\">1</span> <span class=\"hljs-keyword\">else</span> <span class=\"hljs-number\">-1</span><br>     | &#125;<br>defined <span class=\"hljs-class\"><span class=\"hljs-keyword\">class</span> <span class=\"hljs-title\">CustomOrdering</span></span><br><span class=\"hljs-class\"></span><br><span class=\"hljs-class\"><span class=\"hljs-title\">scala&gt;</span> <span class=\"hljs-title\">val</span> <span class=\"hljs-title\">list=List</span>(<span class=\"hljs-params\">(1,&quot;hadoop&quot;</span>),(<span class=\"hljs-params\">1,&quot;xiaokangxxs&quot;</span>),(<span class=\"hljs-params\">1,&quot;azkaban&quot;</span>),(<span class=\"hljs-params\">1,&quot;hive&quot;</span>),(<span class=\"hljs-params\">1,&quot;spark&quot;</span>))</span><br><span class=\"hljs-class\"><span class=\"hljs-title\">list</span></span>: <span class=\"hljs-type\">List</span>[(<span class=\"hljs-type\">Int</span>, <span class=\"hljs-type\">String</span>)] = <span class=\"hljs-type\">List</span>((<span class=\"hljs-number\">1</span>,hadoop), (<span class=\"hljs-number\">1</span>,xiaokangxxs), (<span class=\"hljs-number\">1</span>,azkaban), (<span class=\"hljs-number\">1</span>,hive), (<span class=\"hljs-number\">1</span>,spark))<br><br>scala&gt; <span class=\"hljs-keyword\">implicit</span> <span class=\"hljs-keyword\">val</span> implicitOrdering=<span class=\"hljs-keyword\">new</span> <span class=\"hljs-type\">CustomOrdering</span><br>implicitOrdering: <span class=\"hljs-type\">CustomOrdering</span> = <span class=\"hljs-type\">CustomOrdering</span>@<span class=\"hljs-number\">3102</span>c986<br><br>scala&gt; sc.parallelize(list).takeOrdered(<span class=\"hljs-number\">5</span>)<br>res20: <span class=\"hljs-type\">Array</span>[(<span class=\"hljs-type\">Int</span>, <span class=\"hljs-type\">String</span>)] = <span class=\"hljs-type\">Array</span>((<span class=\"hljs-number\">1</span>,hive), (<span class=\"hljs-number\">1</span>,spark), (<span class=\"hljs-number\">1</span>,hadoop), (<span class=\"hljs-number\">1</span>,azkaban), (<span class=\"hljs-number\">1</span>,xiaokangxxs))<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"2-6-countByKey\"><a href=\"#2-6-countByKey\" class=\"headerlink\" title=\"2.6 countByKey\"></a>2.6 countByKey</h3><p>计算每个键出现的次数：</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>((<span class=\"hljs-string\">&quot;hadoop&quot;</span>,<span class=\"hljs-number\">10</span>),(<span class=\"hljs-string\">&quot;hadoop&quot;</span>,<span class=\"hljs-number\">10</span>),(<span class=\"hljs-string\">&quot;xiaokangxxs&quot;</span>,<span class=\"hljs-number\">3</span>),(<span class=\"hljs-string\">&quot;xiaokang&quot;</span>,<span class=\"hljs-number\">3</span>),(<span class=\"hljs-string\">&quot;azkaban&quot;</span>,<span class=\"hljs-number\">1</span>))<br>scala&gt; sc.parallelize(list).countByKey()<br>res23: scala.collection.<span class=\"hljs-type\">Map</span>[<span class=\"hljs-type\">String</span>,<span class=\"hljs-type\">Long</span>] = <span class=\"hljs-type\">Map</span>(xiaokangxxs -&gt; <span class=\"hljs-number\">1</span>, hadoop -&gt; <span class=\"hljs-number\">2</span>, xiaokang -&gt; <span class=\"hljs-number\">1</span>, azkaban -&gt; <span class=\"hljs-number\">1</span>)<br></code></pre></td></tr></table></figure>\n\n<h3 id=\"2-7-saveAsTextFile\"><a href=\"#2-7-saveAsTextFile\" class=\"headerlink\" title=\"2.7 saveAsTextFile\"></a>2.7 saveAsTextFile</h3><p>将RDD中的元素以文本文件的形式写入本地文件系统、HDFS或其它Hadoop支持的文件系统中。Spark将对每个元素调用toString方法，将元素转换为文本文件中的一行记录。</p>\n<figure class=\"highlight scala\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><code class=\"hljs scala\">scala&gt; <span class=\"hljs-keyword\">val</span> list=<span class=\"hljs-type\">List</span>((<span class=\"hljs-string\">&quot;hadoop&quot;</span>,<span class=\"hljs-number\">10</span>),(<span class=\"hljs-string\">&quot;hadoop&quot;</span>,<span class=\"hljs-number\">10</span>),(<span class=\"hljs-string\">&quot;xiaokangxxs&quot;</span>, <span class=\"hljs-number\">3</span>), (<span class=\"hljs-string\">&quot;spark&quot;</span>, <span class=\"hljs-number\">3</span>), (<span class=\"hljs-string\">&quot;azkaban&quot;</span>, <span class=\"hljs-number\">1</span>))<br>scala&gt; sc.parallelize(list).saveAsTextFile(<span class=\"hljs-string\">&quot;file:///home/xiaokang/file_out&quot;</span>)<br><br>[xiaokang<span class=\"hljs-meta\">@xk</span>1181259634 ~/file_out <span class=\"hljs-number\">18</span>:<span class=\"hljs-number\">03</span>:<span class=\"hljs-number\">23</span>]$cat /home/xiaokang/file_out/part<span class=\"hljs-number\">-00000</span><br>(hadoop,<span class=\"hljs-number\">10</span>)<br>(hadoop,<span class=\"hljs-number\">10</span>)<br>(xiaokangxxs,<span class=\"hljs-number\">3</span>)<br>(spark,<span class=\"hljs-number\">3</span>)<br>(azkaban,<span class=\"hljs-number\">1</span>)<br></code></pre></td></tr></table></figure>\n","categories":[{"name":"大数据","path":"api/categories/大数据.json"}],"tags":[{"name":"Spark","path":"api/tags/Spark.json"},{"name":"RDD","path":"api/tags/RDD.json"},{"name":"算子","path":"api/tags/算子.json"}]}